biggest assumption in the tool: more structural sharing is always better

optional features (disable with #cfg, so binary can be optimized):
- invariant: no formula is in memory twice, so parse with structural sharing or without, reuse (or don't reuse) cached formulas (e.g., then traversal does not need to track visited nodes) - does this actually have any impact? probably it will for negation-CNF
- run NNF before other transformations, or don't run it before (interacts with Plaisted-Greenbaum and structural sharing, as to_nnf creates new sub-expressions)
- let go of unused formulas with Rc or Arc (RefCell needed for internal mutability?) (if we use Rc, we must take care to always store references to "relevant" formulas like different revisions of feature models)
- Plaisted-Greenbaum -> depending on whether equi-countability is preserved/necessary (possibly as a polarity-based variant that does not require NNF)

general:
- multithreaded tree traversal / transformations where possible? or too much overhead / impossible to implement safely? would require abandoning several approaches (e.g., next_id). alternatively: implement multithreading by parallel transformation of separate formulas (e.g., constraints or entire models) and merging them afterwards.
- grep 'todo' in codebase
- is this a knowledge compilation technique? because so much depends on efficient structural sharing - given two neg-CNFs, it is not necessarily efficient to compare them, because they do not usually share structure. i.e., tseitin(phi-a & -phi-b) is probably much better than tseitin(phi-a) & neg-tseitin(phi-b), I suppose. So, this may be less of a knowledge compilation artifact. a formula instance could however be serialized and then contain for example all revisions of a system or all architectures(?) of linux. in that case, we would store a pointer to each relevant formula, tseitin-transform them all, and afterwards we can build arbitrary formulas (e.g., any comparison a & -b) on the roots of those formulas in O(1), and the resulting formulas will only contain relevant constraints (because a & -b does not encode constraints only c refers to). Each step in this pipeline is tractable no matter the size/complexity, intractable is only the NP-complete solving at the end. we could have a compiler then, and a querying tool - the latter can retrieve DIMACS files for any individual version, for comparing versions, for checking whether a given configuration is valid in any version, ... whatever (we can efficiently encode phi, -phi, phi-a (<)=> phi-b, a and b, a or b, a or b or c ... (a big disjunction - was it ever like this?), a and b and c (was it always like this?) without needing a BDD at all). So i describe how to lift Sat-based analyses on entire histories of (or otherwise similar, e.g., architecture in linux?) models. this is something that is very specific for feature models, that we have many similar, but not equal problems to solve, which are related (similar to variational sat, but we are less interested in solving many queries, but solving one query with many internal similarities). possibly even slicing may be possible: as we need phi[x\true] or phi[x\false], and these formulas could be constructed with a lot of reuse (no exponential explosion needed) and then tseitin-transformed (however, slicing many variables may get less nice - don't now if this will be more efficient than FeatureIDE). (is this a good idea? we found that slicing typically does not work so great on Tseitin variables, what happens if I slice a natural variable and create new Tseitin variables for it? is this even an interesting use case??) evolution+slicing could maybe be combined to extract the evolution of a particular subsystem (and, e.g., count it incrementally if counting it directly with PMC/slice+#SAT is impossible).
- maybe this can be combined with presence conditions in some way?
- serde for save/load mechanism?
- criterion.rs for benchmarking
- clap crate for CLI option parsing
- currently, .model files do not include unconstrained features and there is no mechanism to declare them
- currently, re-pivoting with "1 print 2 print" interacts with auxiliary variables (could maybe be 'fixed' by only emitting Aux vars into DIMACS if they are actually mentioned in at least one clause?) also, when i mark features as dead when merging formulas, these dead features will get printed into any dimacs. this is probably not intended. maybe the set of natural variables has to be calculated dynamically when printing DIMACS?
- think carefully when set_child_exprs is correct and when expr is correct - e.g., for NNF transformation, simply using set_child_exprs is invalid, as in -(a|b)&-(c&-(a|b)), we need at the end both (a|b) and (-a&-&b).
- think about whether/when "a.sat to_tseitin b.sat to_tseitin +(1 -2)" is valid (does it require a subsequent tseitin/dist transformation, which is tractable? or must the entire formula be tseitin-transformed afterwards as in "a.sat b.sat to_tseitin +(1 -2)", losing the 'knowledge compilation' aspect to some degree?)
- to improve performance, we may even run the tseitin transformation _while parsing the formula/while constructing it_, although this might require rewriting some code
- for comparing formulas, one assumption is that variables with equal names are considered to be the same (no difference), so renaming cannot be detected obviously. unnamed (aux) variables are not regarded in the comparison (not added as dead variables anywhere), so they better be strictly defined by natural variables (guaranteed by tseitin). best if the input formulas do not include any aux variables.
- sell the tool via invariants/guarantees:
  - associativity (n-ary operators)
  - no idempotency, no commutativity
  - maximum structural sharing
  - minimum aux variables
  - deterministic

formula:
- check if all relevant simplifications are implemented correctly: splicing/merging (and more?), eliminating implies/bi-implies (which may be exponential itself!)
- faster hash function? https://nnethercote.github.io/perf-book/hashing.html https://nnethercote.github.io/2021/12/08/a-brutally-effective-hash-function-in-rust.html as keys are unique, maybe use nohash_hasher?
- for vectors, could use with_capacity to avoid re-allocations (profiling required)
- traversal: the set visited_ids can get large for large formulas for both traversals. maybe it can be compacted in some way. (bit matrix? pre-sized vec<bool> with false as default, that is possible extended when new expressions are created?)
- as an optimization, could combine pre- and postorder traversal to a single DFS that creates NNF on first and distributive CNF on last visit. also, this could be used to avoid calling make_shared after every preorder traversal.
- subtle point regarding NNF: as NNF changes certain subexpressions, it might reduce structural sharing (e.g., on And(Not(Or(a, b)), Or(a, b))). thus, it is beneficial to not run NNF before Tseitin and implement a true polarity-based version of Plaisted-Greenbaum. this does not affect correctness, but probably conciseness of the formula
- randomize clause order? (scrambler?)
- sort children of exprs, this way we take commutativity out of the picture (a&b is equal to b&a) -> more structural sharing means easier comparison of models
- set removed/added variables to be dead so comparison becomes proper (do this in parse_into or somewhere else?)
- to save space, do not print aux variables (print optionally)? or pass an option for that (with configurable prefix?) print into dimacs header which transformation was used / what this file can be used for / which original formula it encodes?

during parsing, before calling .expr/.var, resize the .exprs and .vars vector capacities to the estimated size, so their lookup is faster. (possibly use pest tags to do that.)

tseitin is countpreserving, but so is 'add a new dead variable' or 'add a new core variable' which is why incremental counting/fast reasoning even works.
ignoring abstract features in the comparison might be harder, as slicing them can affect the model count? not sure how to do fast reasoning while ignoring abstract features. maybe rename them to be aux vars? because abstract features are basically tseitin vars (but less/more constrained potentially). ...right?

https://cca.informatik.uni-freiburg.de/sat/ss23/04/
https://cca.informatik.uni-freiburg.de/sat/ss23/05/

what optimizations do featureide and z3 have? can we get faster/smaller formulas?

profile #[inline(always)]

// todo: evaluate this
// let new_hash = Self::hash_expr(&self.exprs[id]);
// if new_hash != old_hash {
//     self.exprs_inv
//         .entry(old_hash)
//         .or_default()
//         .retain(|inner_id| *inner_id != id);
//     if self.exprs_inv.get(&old_hash).unwrap().is_empty() {
//         self.exprs_inv.remove(&old_hash);
//     }
//     self.exprs_inv.entry(new_hash).or_default().push(id);
// }

(for comparison with other formulas)
/// Returns all named sub-variables of this formula.
///
/// Analogously to a sub-expression, a sub-variable is a variable in [Formula::vars] that appear below the auxiliary root expression.
// fn named_vars(&mut self, first_id: Id) -> HashSet<Var<'a>> {
//     let mut named_vars = HashSet::<Var<'a>>::new();
//     self.preorder_rev(first_id, |formula, id| {
//         if let Var(var_id) = formula.exprs[id] {
//             let var_id: usize = var_id.try_into().unwrap();
//             if let Var::Named(_) = formula.vars[var_id] {
//                 named_vars.insert(formula.vars[var_id]);
//             }
//         }
//     });
//     named_vars // todo: when using this set, take care of order to guarantee determinism
// }