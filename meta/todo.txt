biggest assumption in the tool: more structural sharing is always better

optional features (disable with #cfg, so binary can be optimized):
- invariant: no formula is in memory twice, so parse with structural sharing or without, reuse (or don't reuse) cached formulas (e.g., then traversal does not need to track visited nodes) - does this actually have any impact? probably it will for negation-CNF
- run NNF before other transformations, or don't run it before (interacts with Plaisted-Greenbaum and structural sharing, as to_nnf creates new sub-expressions)
- let go of unused formulas with Rc or Arc (RefCell needed for internal mutability?) (if we use Rc, we must take care to always store references to "relevant" formulas like different revisions of feature models)
- Plaisted-Greenbaum -> depending on whether equi-countability is preserved/necessary (possibly as a polarity-based variant that does not require NNF)
- make building var_ids optional

general:
- multithreaded tree traversal / transformations where possible? or too much overhead / impossible to implement safely? would require abandoning several approaches (e.g., next_id). alternatively: implement multithreading by parallel transformation of separate formulas (e.g., constraints or entire models) and merging them afterwards.
- is this a knowledge compilation technique? because so much depends on efficient structural sharing - given two neg-CNFs, it is not necessarily efficient to compare them, because they do not usually share structure. i.e., tseitin(phi-a & -phi-b) is probably much better than tseitin(phi-a) & neg-tseitin(phi-b), I suppose. So, this may be less of a knowledge compilation artifact. a formula instance could however be serialized and then contain for example all revisions of a system or all architectures(?) of linux. in that case, we would store a pointer to each relevant formula, tseitin-transform them all, and afterwards we can build arbitrary formulas (e.g., any comparison a & -b) on the roots of those formulas in O(1), and the resulting formulas will only contain relevant constraints (because a & -b does not encode constraints only c refers to). Each step in this pipeline is tractable no matter the size/complexity, intractable is only the NP-complete solving at the end. we could have a compiler then, and a querying tool - the latter can retrieve DIMACS files for any individual version, for comparing versions, for checking whether a given configuration is valid in any version, ... whatever (we can efficiently encode phi, -phi, phi-a (<)=> phi-b, a and b, a or b, a or b or c ... (a big disjunction - was it ever like this?), a and b and c (was it always like this?) without needing a BDD at all). So i describe how to lift Sat-based analyses on entire histories of (or otherwise similar, e.g., architecture in linux?) models. this is something that is very specific for feature models, that we have many similar, but not equal problems to solve, which are related (similar to variational sat, but we are less interested in solving many queries, but solving one query with many internal similarities). possibly even slicing may be possible: as we need phi[x\true] or phi[x\false], and these formulas could be constructed with a lot of reuse (no exponential explosion needed) and then tseitin-transformed (however, slicing many variables may get less nice - don't now if this will be more efficient than FeatureIDE). (is this a good idea? we found that slicing typically does not work so great on Tseitin variables, what happens if I slice a natural variable and create new Tseitin variables for it? is this even an interesting use case??) evolution+slicing could maybe be combined to extract the evolution of a particular subsystem (and, e.g., count it incrementally if counting it directly with PMC/slice+#SAT is impossible).
- maybe this can be combined with presence conditions in some way?
- serde for save/load mechanism?
- criterion.rs for benchmarking
- clap crate for CLI option parsing
- currently, .model files do not include unconstrained features and there is no mechanism to declare them
- currently, re-pivoting with "1 print 2 print" interacts with auxiliary variables (could maybe be 'fixed' by only emitting Aux vars into DIMACS if they are actually mentioned in at least one clause?) also, when i mark features as dead when merging formulas, these dead features will get printed into any dimacs. this is probably not intended. maybe the set of natural variables has to be calculated dynamically when printing DIMACS?
- think carefully when set_child_exprs is correct and when expr is correct - e.g., for NNF transformation, simply using set_child_exprs is invalid, as in -(a|b)&-(c&-(a|b)), we need at the end both (a|b) and (-a&-&b).
- think about whether/when "a.sat to_tseitin b.sat to_tseitin +(1 -2)" is valid (does it require a subsequent tseitin/dist transformation, which is tractable? or must the entire formula be tseitin-transformed afterwards as in "a.sat b.sat to_tseitin +(1 -2)", losing the 'knowledge compilation' aspect to some degree? is this even correct - are the individual tseitin vars still determinate?) possibly extend .expr/.set_expr to look up already tseitin-abbreviated expressions (to make tseitin idempotent and allow a.model to_tseitin b.model to_tseitin *(-1 2)). eg, with a dict from expr id to aux var id. also, use def_and in sat_inline parser for adding dead variables, so if children have already been tseitin transformed, nothing needs to be done.
- to improve performance, we may even run the tseitin transformation _while parsing the formula/while constructing it_, although this might require rewriting some code
- for comparing formulas, one assumption is that variables with equal names are considered to be the same (no difference), so renaming cannot be detected obviously. unnamed (aux) variables are not regarded in the comparison (not added as dead variables anywhere), so they better be strictly defined by natural variables (guaranteed by tseitin). best if the input formulas do not include any aux variables. another way to see it is that we check whether two formulas are equivalent given a bijection of their variable names, and do not check if there is any bijection (which would be necessary to ignore renaming of features).
- sell the tool via invariants/guarantees:
  - associativity (n-ary operators)
  - no idempotency, no commutativity
  - maximum structural sharing
  - minimum aux variables
  - deterministic
- do we have formulas where IDs might overflow (esp. in simp_expr)?
- implement reasoning about FM edits 1.0
- implement time measurements for phases
- implement slicing by repurposing named variables as tseitin variables?

formula:
- eliminating implies/bi-implies (which may be exponential itself!)
- faster hash function? https://nnethercote.github.io/perf-book/hashing.html https://nnethercote.github.io/2021/12/08/a-brutally-effective-hash-function-in-rust.html as keys are unique, maybe use nohash_hasher?
- for vectors, could use with_capacity to avoid re-allocations (profiling required)
- traversal: the set visited_ids can get large for large formulas for both traversals. maybe it can be compacted in some way. (bit matrix? pre-sized vec<bool> with false as default, that is possible extended when new expressions are created?)
- subtle point regarding NNF: as NNF changes certain subexpressions, it might reduce structural sharing (e.g., on And(Not(Or(a, b)), Or(a, b))). thus, it is beneficial to not run NNF before Tseitin and implement a true polarity-based version of Plaisted-Greenbaum. this does not affect correctness, but probably conciseness of the formula
- randomize clause order? (scrambler?)
- sort children of exprs, this way we take commutativity out of the picture (a&b is equal to b&a) -> more structural sharing means easier comparison of models
- set removed/added variables to be dead so comparison becomes proper (do this in parse_into or somewhere else?)
- to save space, do not print aux variables (print optionally)? or pass an option for that (with configurable prefix?) print into dimacs header which transformation was used / what this file can be used for / which original formula it encodes? also, for satisfy and enumerate, possibly hide aux variables.
- remove hardcoded "NewRootFeature" from io.jar, which is only used for ignoring synthetic roots during slicing. also, there's more in-band signaling (replacing = with "__EQUALS__", using "," to separate sliced features)

during parsing, before calling .expr/.var, resize the .exprs and .vars vector capacities to the estimated size, so their lookup is faster. (possibly use pest tags to do that.)

tseitin is countpreserving, but so is 'add a new dead variable' or 'add a new core variable' which is why incremental counting/fast reasoning even works.
ignoring abstract features in the comparison might be harder, as slicing them can affect the model count? not sure how to do fast reasoning while ignoring abstract features. maybe rename them to be aux vars? because abstract features are basically tseitin vars (but less/more constrained potentially). ...right?

https://cca.informatik.uni-freiburg.de/sat/ss23/04/
https://cca.informatik.uni-freiburg.de/sat/ss23/05/

what optimizations does z3 have? can we get faster/smaller formulas?

one more: the negation operator Not could be completely avoided by encoding it in the sign of child IDs. this would save memory and reduce the number of lookups (might be particularly interesting for implementing polarity-based Plaisted-Greenbaum). however, this would also make the code much harder to read and probably requires introducing an auxiliary root again.

profile #[inline(always)]

evaluate this:
// let new_hash = Self::hash_expr(&self.exprs[id]);
// if new_hash != old_hash {
//     self.exprs_inv
//         .entry(old_hash)
//         .or_default()
//         .retain(|inner_id| *inner_id != id);
//     if self.exprs_inv.get(&old_hash).unwrap().is_empty() {
//         self.exprs_inv.remove(&old_hash);
//     }
//     self.exprs_inv.entry(new_hash).or_default().push(id);
// }

- hybrid visitor that decides for the dist or tseitin visitor (one criterion: could multiply all len's to calculate a threshold for hybrid tseitin, as done in z3)
- reproduce ASE (hybrid parameter should influence solving performance)
- measure time + max heap size
- measure time of parsing, preprocessing, and DPLL/CDCL (instrument solvers?)

- count diffs of models in parallel
- teile jeden arbitrary edit in eine generalisierung und eine Spezialisierung auf, indem syntaktisch die entfernten und zugefügten klauseln getrennt werden. dann wird je eine diffzahl auf 0 forciert. vmtl liegt die Schwierigkeit darin, die gemeinsamen klauseln zu identifizieren, das sollte aber mit structural sharing leicht sein. slicing ist vmtl immer noch nötig, aber einfacher bei kleinen windows
- move parsed file and extension into formula instance as optional
- crashes: cargo run ~/Downloads/v2.5.45\[alpha\].model to_canon print to_cnf_dist to_clauses countcargo run ~/Downloads/v2.5.46\[i386\].uvl to_cnf_dist assert_count